{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "##LCZ##2021##"
      ],
      "metadata": {
        "id": "dTOv2Knw7jfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "import pandas as pd\n",
        "\n",
        "# Confusion matrix data from user\n",
        "matrix_data = [\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,11,1,0,0,0,0,0,0,3,1,1],\n",
        "    [0,1,339,0,23,1,0,0,0,16,5,0,0],\n",
        "    [0,0,9,22,1,0,0,0,0,12,1,1,0],\n",
        "    [0,0,15,0,130,0,0,0,0,15,0,0,0],\n",
        "    [0,0,19,0,1,11,0,0,0,0,5,1,1],\n",
        "    [0,0,1,3,2,0,317,13,1,32,1,0,2],\n",
        "    [0,0,1,1,0,0,37,67,0,15,1,0,0],\n",
        "    [0,0,0,0,2,0,22,5,7,4,0,0,0],\n",
        "    [0,0,3,0,9,0,5,0,0,1540,2,0,0],\n",
        "    [0,0,6,2,0,0,1,0,0,3,62,1,1],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,654,2],\n",
        "    [0,0,0,0,0,0,0,0,0,2,0,10,4056]\n",
        "]\n",
        "\n",
        "overall_accuracy = 0.9557029177718833\n",
        "kappa_coefficient = 0.931911901404142\n",
        "\n",
        "# Create DataFrame for table\n",
        "df = pd.DataFrame(matrix_data)\n",
        "\n",
        "# Create document\n",
        "doc = Document()\n",
        "doc.add_heading('Accuracy Assessment Report for LCZ Classification_2021', level=1)\n",
        "\n",
        "# Introduction\n",
        "doc.add_paragraph(\n",
        "    \"This report explains the results of the accuracy assessment for the LCZ classification. \"\n",
        "    \"It includes the confusion matrix, overall accuracy, and Kappa coefficient, along with formulas and interpretation.\"\n",
        ")\n",
        "\n",
        "# Add confusion matrix\n",
        "doc.add_heading('1. Confusion Matrix', level=2)\n",
        "doc.add_paragraph(\n",
        "    \"The confusion matrix compares predicted class labels against reference (true) class labels. \"\n",
        "    \"Rows correspond to the actual classes, and columns correspond to the predicted classes.\"\n",
        ")\n",
        "\n",
        "# Add table to DOCX\n",
        "table = doc.add_table(rows=len(df)+1, cols=len(df.columns)+1)\n",
        "table.style = 'Table Grid'\n",
        "\n",
        "# Header row\n",
        "table.cell(0, 0).text = 'Actual \\\\ Predicted'\n",
        "for j in range(len(df.columns)):\n",
        "    table.cell(0, j+1).text = str(j)\n",
        "\n",
        "# Data rows\n",
        "for i in range(len(df)):\n",
        "    table.cell(i+1, 0).text = str(i)\n",
        "    for j in range(len(df.columns)):\n",
        "        table.cell(i+1, j+1).text = str(df.iloc[i, j])\n",
        "\n",
        "# Add OA and Kappa\n",
        "doc.add_heading('2. Overall Accuracy (OA)', level=2)\n",
        "doc.add_paragraph(f\"Overall Accuracy (OA) measures the proportion of correctly classified instances:\\n\\n\"\n",
        "                  f\"OA = (Σ correct classifications) / (Total samples)\\n\\n\"\n",
        "                  f\"Here, OA = {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
        "\n",
        "doc.add_heading('3. Kappa Coefficient (κ)', level=2)\n",
        "doc.add_paragraph(\n",
        "    \"The Kappa coefficient measures classification accuracy adjusted for chance agreement:\\n\\n\"\n",
        "    \"κ = (Po - Pe) / (1 - Pe)\\n\\n\"\n",
        "    \"Where Po is the observed agreement (OA) and Pe is the expected agreement by chance.\\n\"\n",
        "    f\"Here, κ = {kappa_coefficient:.4f}\"\n",
        ")\n",
        "\n",
        "doc.add_heading('4. Interpretation', level=2)\n",
        "doc.add_paragraph(\n",
        "    \"According to Landis & Koch (1977):\\n\"\n",
        "    \"- < 0: Poor agreement\\n\"\n",
        "    \"- 0.01–0.20: Slight agreement\\n\"\n",
        "    \"- 0.21–0.40: Fair agreement\\n\"\n",
        "    \"- 0.41–0.60: Moderate agreement\\n\"\n",
        "    \"- 0.61–0.80: Substantial agreement\\n\"\n",
        "    \"- 0.81–1.00: Almost perfect agreement\\n\\n\"\n",
        "    \"Since κ ≈ 0.93, the classification shows **Almost Perfect Agreement**.\"\n",
        ")\n",
        "\n",
        "# Save DOCX\n",
        "output_path = '/content/LCZ_Accuracy_Assessment_with_Matrix_2021.docx'\n",
        "doc.save(output_path)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5NM6WwaH4Gh8",
        "outputId": "79555542-190c-4ebc-87b5-d8c3383d2b79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LCZ_Accuracy_Assessment_with_Matrix_2021.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##LCZ##2011##"
      ],
      "metadata": {
        "id": "brA4VB0D7nDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "import pandas as pd\n",
        "\n",
        "# Confusion matrix data from user\n",
        "matrix_data = [\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,18,0,0,2,0,0,0,0,0,0,0],\n",
        "    [0,0,129,0,1,2,0,0,0,0,0,0,0],\n",
        "    [0,0,1,15,10,1,2,3,0,18,0,0,0],\n",
        "    [0,0,0,0,100,0,0,0,0,9,0,0,0],\n",
        "    [0,0,10,0,0,13,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,4,0,352,2,0,54,2,0,4],\n",
        "    [0,0,0,0,7,0,11,64,0,17,0,0,0],\n",
        "    [0,0,0,0,0,0,14,0,5,13,0,0,0],\n",
        "    [0,0,0,0,1,0,11,1,0,1602,5,0,0],\n",
        "    [0,0,9,3,3,4,2,0,0,3,175,1,5],\n",
        "    [0,0,0,0,0,0,0,0,0,1,0,640,84],\n",
        "    [0,0,3,1,2,0,4,1,0,10,2,56,2578]\n",
        "]\n",
        "\n",
        "overall_accuracy = 0.9315270935960591\n",
        "kappa_coefficient = 0.9039508579135105\n",
        "# Create DataFrame for table\n",
        "df = pd.DataFrame(matrix_data)\n",
        "\n",
        "# Create document\n",
        "doc = Document()\n",
        "doc.add_heading('Accuracy Assessment Report for LCZ Classification_2011', level=1)\n",
        "\n",
        "# Introduction\n",
        "doc.add_paragraph(\n",
        "    \"This report explains the results of the accuracy assessment for the LCZ classification. \"\n",
        "    \"It includes the confusion matrix, overall accuracy, and Kappa coefficient, along with formulas and interpretation.\"\n",
        ")\n",
        "\n",
        "# Add confusion matrix\n",
        "doc.add_heading('1. Confusion Matrix', level=2)\n",
        "doc.add_paragraph(\n",
        "    \"The confusion matrix compares predicted class labels against reference (true) class labels. \"\n",
        "    \"Rows correspond to the actual classes, and columns correspond to the predicted classes.\"\n",
        ")\n",
        "\n",
        "# Add table to DOCX\n",
        "table = doc.add_table(rows=len(df)+1, cols=len(df.columns)+1)\n",
        "table.style = 'Table Grid'\n",
        "\n",
        "# Header row\n",
        "table.cell(0, 0).text = 'Actual \\\\ Predicted'\n",
        "for j in range(len(df.columns)):\n",
        "    table.cell(0, j+1).text = str(j)\n",
        "\n",
        "# Data rows\n",
        "for i in range(len(df)):\n",
        "    table.cell(i+1, 0).text = str(i)\n",
        "    for j in range(len(df.columns)):\n",
        "        table.cell(i+1, j+1).text = str(df.iloc[i, j])\n",
        "\n",
        "# Add OA and Kappa\n",
        "doc.add_heading('2. Overall Accuracy (OA)', level=2)\n",
        "doc.add_paragraph(f\"Overall Accuracy (OA) measures the proportion of correctly classified instances:\\n\\n\"\n",
        "                  f\"OA = (Σ correct classifications) / (Total samples)\\n\\n\"\n",
        "                  f\"Here, OA = {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
        "\n",
        "doc.add_heading('3. Kappa Coefficient (κ)', level=2)\n",
        "doc.add_paragraph(\n",
        "    \"The Kappa coefficient measures classification accuracy adjusted for chance agreement:\\n\\n\"\n",
        "    \"κ = (Po - Pe) / (1 - Pe)\\n\\n\"\n",
        "    \"Where Po is the observed agreement (OA) and Pe is the expected agreement by chance.\\n\"\n",
        "    f\"Here, κ = {kappa_coefficient:.4f}\"\n",
        ")\n",
        "\n",
        "doc.add_heading('4. Interpretation', level=2)\n",
        "doc.add_paragraph(\n",
        "    \"According to Landis & Koch (1977):\\n\"\n",
        "    \"- < 0: Poor agreement\\n\"\n",
        "    \"- 0.01–0.20: Slight agreement\\n\"\n",
        "    \"- 0.21–0.40: Fair agreement\\n\"\n",
        "    \"- 0.41–0.60: Moderate agreement\\n\"\n",
        "    \"- 0.61–0.80: Substantial agreement\\n\"\n",
        "    \"- 0.81–1.00: Almost perfect agreement\\n\\n\"\n",
        "    \"Since κ ≈ 0.93, the classification shows **Almost Perfect Agreement**.\"\n",
        ")\n",
        "\n",
        "# Save DOCX\n",
        "output_path = '/content/LCZ_Accuracy_Assessment_with_Matrix_2011.docx'\n",
        "doc.save(output_path)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x8GfVlFl6Ago",
        "outputId": "61f7b91b-ab95-42f4-e250-a29c37eb211d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LCZ_Accuracy_Assessment_with_Matrix_2011.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}